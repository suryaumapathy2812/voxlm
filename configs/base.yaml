# =============================================================================
# VoxLM Base Configuration
# =============================================================================
# This file contains default values inherited by all model configs.
# Override specific values in model-specific configs (e.g., voxlm-2b.yaml)
#
# Usage:
#   ./scripts/quick_train.sh configs/voxlm-2b.yaml
#   docker-compose run --rm voxlm configs/voxlm-2b.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model:
  # Model variant name (used for checkpoint naming)
  name: "voxlm-2b"
  
  # Architecture version: "v2" uses alignment module for timestamps
  architecture_version: "v2"
  
  # Audio Encoder (Whisper, IndicWhisper, MMS, etc.)
  audio_encoder: "openai/whisper-small"
  audio_encoder_dim: 768
  freeze_audio_encoder: true
  
  # LLM Backbone (Qwen, Llama, Phi, Gemma, etc.)
  llm_model: "Qwen/Qwen2-1.5B"
  llm_dim: 1536
  freeze_llm: true
  
  # LoRA Configuration (efficient fine-tuning)
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  
  # Projection Layer
  projection_dropout: 0.1
  downsample_factor: 4  # 50Hz -> 12.5Hz
  
  # Alignment Module (for word-level timestamps)
  alignment_num_layers: 2
  alignment_num_heads: 8
  alignment_dropout: 0.1
  alignment_loss_weight: 1.0
  monotonicity_loss_weight: 0.1

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------
training:
  # Phase 1: Basic transcription training
  phase1:
    epochs: 10
    batch_size: 8
    learning_rate: 1.0e-4
    warmup_steps: 1000
    gradient_accumulation: 1
    
  # Phase 2: Timestamp alignment training
  phase2:
    epochs: 5
    batch_size: 16
    learning_rate: 5.0e-5
    alignment_weight: 1.0
  
  # Common settings
  max_audio_length: 30  # seconds
  max_text_length: 448  # tokens
  
  # Hardware optimization
  use_amp: true  # Automatic Mixed Precision (bfloat16)
  compile_model: false  # torch.compile() - enable for production
  num_workers: 8  # DataLoader workers (0 for macOS)

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  # Data directory (contains LibriSpeech/)
  data_dir: "./data"
  
  # LibriSpeech splits
  train_split: "train-clean-100"
  val_split: "dev-clean"
  
  # Timestamps directory (generated by generate_timestamps.py)
  timestamps_dir: "./data/timestamps"
  
  # Timestamp generation settings
  whisper_model: "small"  # For generating timestamps
  max_wer: 0.1  # Filter samples with WER > 10%

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
output:
  # Checkpoint directory
  checkpoint_dir: "./checkpoints"
  
  # Final model output (after training completes)
  final_model_dir: "./output/models"
  
  # Logging
  use_wandb: false
  wandb_project: "voxlm"

# -----------------------------------------------------------------------------
# Device Configuration
# -----------------------------------------------------------------------------
device:
  # "auto" = use CUDA if available, else CPU
  type: "auto"
  
  # For multi-GPU (future)
  # gpu_ids: [0, 1]
