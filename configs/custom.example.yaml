# =============================================================================
# VoxLM Custom Configuration Template
# =============================================================================
# Copy this file and customize for your use case.
#
# Usage:
#   cp configs/custom.yaml.example configs/my-model.yaml
#   # Edit my-model.yaml with your settings
#   ./scripts/quick_train.sh configs/my-model.yaml
# =============================================================================

# Inherit from base config (optional - you can override everything)
_base_: "base.yaml"

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model:
  # Give your model a unique name
  name: "voxlm-custom"
  
  # ==========================================================================
  # AUDIO ENCODER OPTIONS
  # ==========================================================================
  # Choose one of these audio encoders:
  #
  # Whisper (OpenAI) - Best general-purpose:
  #   "openai/whisper-tiny"      dim=384   (39M params)
  #   "openai/whisper-base"      dim=512   (74M params)
  #   "openai/whisper-small"     dim=768   (244M params)
  #   "openai/whisper-medium"    dim=1024  (769M params)
  #   "openai/whisper-large-v3"  dim=1280  (1.5B params, 99 languages)
  #
  # IndicWhisper (AI4Bharat) - Indian languages:
  #   "ai4bharat/indicwhisper-large-v2"  dim=1280  (12+ Indian languages)
  #
  # MMS (Meta) - 1000+ languages:
  #   "facebook/mms-1b-all"      dim=1280  (1000+ languages)
  # ==========================================================================
  audio_encoder: "openai/whisper-small"
  audio_encoder_dim: 768
  freeze_audio_encoder: true
  
  # ==========================================================================
  # LLM BACKBONE OPTIONS
  # ==========================================================================
  # Choose one of these LLMs:
  #
  # Qwen (Alibaba) - Excellent multilingual:
  #   "Qwen/Qwen2-0.5B"          dim=896   (0.5B params)
  #   "Qwen/Qwen2-1.5B"          dim=1536  (1.5B params)
  #   "Qwen/Qwen2.5-3B"          dim=2048  (3B params)
  #   "Qwen/Qwen2.5-7B"          dim=3584  (7B params)
  #
  # Llama (Meta) - Strong English:
  #   "meta-llama/Llama-3.1-8B-Instruct"  dim=4096  (8B params)
  #
  # Phi (Microsoft) - Efficient:
  #   "microsoft/phi-4"          dim=5120  (14B params)
  #
  # Gemma (Google) - Balanced:
  #   "google/gemma-2-9b-it"     dim=3584  (9B params)
  # ==========================================================================
  llm_model: "Qwen/Qwen2-1.5B"
  llm_dim: 1536
  freeze_llm: true
  
  # LoRA Configuration
  # - lora_r: Rank (8-64, higher = more capacity but slower)
  # - lora_alpha: Scaling (typically 2x lora_r)
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  
  # Alignment Module
  alignment_num_layers: 2
  alignment_num_heads: 8

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------
training:
  # Phase 1: Basic transcription
  phase1:
    epochs: 10
    batch_size: 8
    learning_rate: 1.0e-4
    warmup_steps: 1000
    gradient_accumulation: 1  # Increase if OOM
    
  # Phase 2: Timestamp alignment
  phase2:
    epochs: 5
    batch_size: 16
    learning_rate: 5.0e-5
    alignment_weight: 1.0
  
  # Hardware
  use_amp: true
  compile_model: false
  num_workers: 8

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  # Path to your data directory
  data_dir: "./data"
  
  # LibriSpeech splits (or your custom dataset)
  train_split: "train-clean-100"
  val_split: "dev-clean"
  
  # Timestamps (generated by generate_timestamps.py)
  timestamps_dir: "./data/timestamps"
  
  # Whisper model for timestamp generation
  whisper_model: "small"
  max_wer: 0.1

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
output:
  # Where to save checkpoints during training
  checkpoint_dir: "./checkpoints"
  
  # Where to save final model after training
  final_model_dir: "./output/models"
  
  # Weights & Biases logging
  use_wandb: false
  wandb_project: "voxlm"

# -----------------------------------------------------------------------------
# Device Configuration
# -----------------------------------------------------------------------------
device:
  type: "auto"  # "auto", "cuda", "cpu"
