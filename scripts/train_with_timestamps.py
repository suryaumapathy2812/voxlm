#!/usr/bin/env python3
"""
Phase 2 Training: Train VoxLM with timestamp supervision.

This script trains the alignment module using word-level timestamps
generated by WhisperX or MFA.

Usage:
    # Using YAML config (recommended)
    python scripts/train_with_timestamps.py \
        --config configs/voxlm-2b.yaml \
        --checkpoint checkpoints/voxlm-2b/best.pt

    # Using CLI arguments (legacy)
    python scripts/train_with_timestamps.py \
        --model voxlm-2b \
        --checkpoint checkpoints/voxlm-2b/best.pt \
        --timestamps data/timestamps/train-clean-100/timestamps_manifest.json \
        --val-timestamps data/timestamps/dev-clean/timestamps_manifest.json \
        --epochs 5
        
    # Mix: YAML config with CLI overrides
    python scripts/train_with_timestamps.py \
        --config configs/voxlm-2b.yaml \
        --checkpoint checkpoints/voxlm-2b/best.pt \
        --epochs 10
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import GradScaler, autocast
import torchaudio
from tqdm import tqdm

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import get_config, MODEL_CONFIGS, load_full_config
from src.model import VoxLM


def setup_torch_optimizations():
    """Enable PyTorch optimizations for H100/modern GPUs."""
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"GPU: {gpu_name} ({gpu_mem:.1f} GB)")


class TimestampDataset(Dataset):
    """
    Dataset with word-level timestamps for alignment training.

    Expects manifest format:
    [
        {
            "id": "1272-128104-0000",
            "audio": "/path/to/audio.flac",
            "text": "mister quilter is the apostle...",
            "timestamps": [
                {"word": "mister", "start": 0.12, "end": 0.45},
                {"word": "quilter", "start": 0.45, "end": 0.89},
                ...
            ]
        },
        ...
    ]
    """

    def __init__(
        self,
        manifest_path: str,
        max_audio_length: int = 30,
        sample_rate: int = 16000,
    ):
        self.manifest_path = Path(manifest_path)
        self.max_audio_length = max_audio_length
        self.sample_rate = sample_rate
        self.max_samples = max_audio_length * sample_rate

        # Load manifest
        with open(manifest_path) as f:
            self.samples = json.load(f)

        print(
            f"Loaded {len(self.samples)} samples with timestamps from {manifest_path}"
        )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Load audio
        try:
            import soundfile as sf

            audio_data, sr = sf.read(sample["audio"])
            waveform = torch.from_numpy(audio_data).float()

            if waveform.dim() > 1:
                waveform = waveform.mean(dim=1)

            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                waveform = resampler(waveform.unsqueeze(0)).squeeze(0)

            # Truncate or pad
            if waveform.shape[0] > self.max_samples:
                waveform = waveform[: self.max_samples]
            elif waveform.shape[0] < self.max_samples:
                pad = torch.zeros(self.max_samples - waveform.shape[0])
                waveform = torch.cat([waveform, pad])

        except Exception as e:
            print(f"Error loading {sample['audio']}: {e}")
            waveform = torch.zeros(self.max_samples)

        # Extract timestamps as list of (start, end) tuples
        timestamps = [(w["start"], w["end"]) for w in sample.get("timestamps", [])]

        return {
            "audio": waveform,
            "text": sample["text"],
            "timestamps": timestamps,
        }


class TimestampCollateFn:
    """Collate function that handles variable-length timestamps.

    CRITICAL: Expands word-level timestamps to token-level timestamps.
    This is necessary because:
    - Whisper generates word-level timestamps
    - But the model operates on subword tokens
    - Each word may be split into multiple tokens
    """

    def __init__(self, tokenizer, max_text_length=448, add_eos=True):
        self.tokenizer = tokenizer
        self.max_text_length = max_text_length
        self.add_eos = add_eos

        if self.tokenizer.eos_token is None:
            self.tokenizer.eos_token = "<|endoftext|>"

    def _expand_word_timestamps_to_tokens(
        self,
        text: str,
        word_timestamps: List[Tuple[float, float]],
        token_ids: torch.Tensor,
    ) -> List[Tuple[float, float]]:
        """
        Expand word-level timestamps to token-level timestamps.

        Args:
            text: The full text (without EOS)
            word_timestamps: List of (start, end) for each word
            token_ids: Token IDs from tokenizer

        Returns:
            List of (start, end) for each token
        """
        # Split text into words
        words = text.split()

        if len(words) != len(word_timestamps):
            # Mismatch - fall back to even distribution
            # This can happen if Whisper's word segmentation differs from simple split
            num_tokens = len(token_ids)
            if len(word_timestamps) > 0:
                total_duration = word_timestamps[-1][1]
                token_duration = total_duration / num_tokens
                return [
                    (i * token_duration, (i + 1) * token_duration)
                    for i in range(num_tokens)
                ]
            else:
                return [(0.0, 0.0) for _ in range(num_tokens)]

        # Build token-to-word mapping by tokenizing each word
        token_timestamps = []
        current_token_idx = 0

        for word_idx, word in enumerate(words):
            # Get timestamp for this word
            if word_idx < len(word_timestamps):
                start, end = word_timestamps[word_idx]
            else:
                start, end = 0.0, 0.0

            # Tokenize this word (with space prefix for non-first words)
            if word_idx == 0:
                word_tokens = self.tokenizer.encode(word, add_special_tokens=False)
            else:
                # Add space prefix for subsequent words (how Qwen tokenizer works)
                word_tokens = self.tokenizer.encode(
                    " " + word, add_special_tokens=False
                )

            # Assign the same timestamp to all tokens of this word
            for _ in word_tokens:
                token_timestamps.append((start, end))
                current_token_idx += 1

        # Handle EOS token if present
        if self.add_eos:
            # EOS gets the timestamp of the last word's end
            if word_timestamps:
                last_end = word_timestamps[-1][1]
                token_timestamps.append((last_end, last_end))
            else:
                token_timestamps.append((0.0, 0.0))

        # Pad or truncate to match actual token count
        actual_token_count = len(token_ids)
        if len(token_timestamps) < actual_token_count:
            # Pad with last timestamp
            last_ts = token_timestamps[-1] if token_timestamps else (0.0, 0.0)
            while len(token_timestamps) < actual_token_count:
                token_timestamps.append(last_ts)
        elif len(token_timestamps) > actual_token_count:
            token_timestamps = token_timestamps[:actual_token_count]

        return token_timestamps

    def __call__(self, batch):
        audios = torch.stack([b["audio"] for b in batch])

        # Get original texts (without EOS) and word timestamps
        original_texts = [b["text"] for b in batch]
        word_timestamps_batch = [b["timestamps"] for b in batch]

        # Tokenize texts with EOS
        texts = (
            [t + self.tokenizer.eos_token for t in original_texts]
            if self.add_eos
            else original_texts
        )

        tokens = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=self.max_text_length,
            return_tensors="pt",
        )

        # Expand word timestamps to token timestamps for each sample
        token_timestamps_batch = []
        for i, (text, word_ts, token_ids) in enumerate(
            zip(original_texts, word_timestamps_batch, tokens.input_ids)
        ):
            # Get non-padded token count
            non_pad_mask = token_ids != self.tokenizer.pad_token_id
            non_pad_count = non_pad_mask.sum().item()

            # Expand word timestamps to token timestamps
            token_ts = self._expand_word_timestamps_to_tokens(
                text, word_ts, token_ids[:non_pad_count]
            )

            # Pad to full sequence length
            while len(token_ts) < len(token_ids):
                token_ts.append((0.0, 0.0))

            token_timestamps_batch.append(token_ts)

        return {
            "audio": audios,
            "labels": tokens.input_ids,
            "texts": texts,
            "token_timestamps": token_timestamps_batch,
        }


class TimestampTrainer:
    """Trainer for Phase 2: alignment training with timestamps."""

    def __init__(
        self,
        model: VoxLM,
        train_loader: DataLoader,
        val_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        scheduler,
        device: str = "cuda",
        output_dir: str = "checkpoints",
        use_amp: bool = True,
        alignment_loss_weight: float = 1.0,
    ):
        self.device = device
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.use_amp = use_amp and torch.cuda.is_available()
        self.alignment_loss_weight = alignment_loss_weight

        self.scaler = GradScaler(enabled=self.use_amp)
        self.global_step = 0
        self.best_val_loss = float("inf")

        if self.use_amp:
            print("Mixed precision training enabled (bfloat16)")

    def train_epoch(self, epoch: int):
        self.model.train()
        total_loss = 0
        total_ce_loss = 0
        total_align_loss = 0
        num_batches = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}")
        for batch in pbar:
            audio = batch["audio"].to(self.device, non_blocking=True)
            labels = batch["labels"].to(self.device, non_blocking=True)
            token_timestamps = batch["token_timestamps"]  # List of lists

            # Debug: Print first batch info to verify timestamp expansion
            if num_batches == 0 and epoch == 0:
                print(f"\nDEBUG: First batch timestamp info:")
                print(f"  Labels shape: {labels.shape}")
                print(f"  Num token_timestamps: {len(token_timestamps)}")
                if token_timestamps:
                    print(
                        f"  First sample timestamps count: {len(token_timestamps[0])}"
                    )
                    print(f"  First 5 token timestamps: {token_timestamps[0][:5]}")
                    # Decode first few tokens to verify alignment
                    first_tokens = labels[0][:10].tolist()
                    print(f"  First 10 token IDs: {first_tokens}")
                    print(
                        f"  First 10 tokens decoded: {[self.model.tokenizer.decode([t]) for t in first_tokens]}"
                    )

            with autocast(dtype=torch.bfloat16, enabled=self.use_amp):
                outputs = self.model(
                    audio=audio,
                    labels=labels,
                    token_timestamps=token_timestamps,
                    return_alignment=True,
                )
                loss = outputs.loss

            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad(set_to_none=True)
            self.scheduler.step()

            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1

            pbar.set_postfix(
                {
                    "loss": loss.item(),
                    "lr": self.scheduler.get_last_lr()[0],
                }
            )

        return total_loss / max(num_batches, 1)

    @torch.no_grad()
    def validate(self):
        self.model.eval()
        total_loss = 0
        num_batches = 0

        for batch in tqdm(self.val_loader, desc="Validating"):
            audio = batch["audio"].to(self.device, non_blocking=True)
            labels = batch["labels"].to(self.device, non_blocking=True)
            token_timestamps = batch["token_timestamps"]

            with autocast(dtype=torch.bfloat16, enabled=self.use_amp):
                outputs = self.model(
                    audio=audio,
                    labels=labels,
                    token_timestamps=token_timestamps,
                    return_alignment=True,
                )

            total_loss += outputs.loss.item()
            num_batches += 1

        return total_loss / max(num_batches, 1)

    def save_checkpoint(self, epoch: int, val_loss: float):
        checkpoint = {
            "epoch": epoch,
            "global_step": self.global_step,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "val_loss": val_loss,
            "config": self.model.config,
            "phase": "phase2_timestamps",
        }

        torch.save(checkpoint, self.output_dir / "latest_phase2.pt")

        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            torch.save(checkpoint, self.output_dir / "best_phase2.pt")
            print(f"New best model saved with val_loss={val_loss:.4f}")

    def train(self, num_epochs: int):
        for epoch in range(num_epochs):
            train_loss = self.train_epoch(epoch)
            val_loss = self.validate()

            print(
                f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}"
            )
            self.save_checkpoint(epoch, val_loss)


def main():
    parser = argparse.ArgumentParser(description="Phase 2: Train VoxLM with timestamps")

    # YAML config option (recommended)
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to YAML config file (e.g., configs/voxlm-2b.yaml). CLI args override config values.",
    )

    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Model variant. Ignored if --config is provided.",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="Phase 1 checkpoint to continue from",
    )
    parser.add_argument(
        "--timestamps", type=str, default=None, help="Training timestamps manifest"
    )
    parser.add_argument(
        "--val-timestamps",
        type=str,
        default=None,
        help="Validation timestamps manifest",
    )
    parser.add_argument("--output", type=str, default=None, help="Output directory")
    parser.add_argument("--epochs", type=int, default=None)
    parser.add_argument("--batch-size", type=int, default=None)
    parser.add_argument(
        "--lr", type=float, default=None, help="Lower LR for fine-tuning"
    )
    parser.add_argument(
        "--alignment-weight", type=float, default=None, help="Weight for alignment loss"
    )
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--num-workers", type=int, default=None)

    args = parser.parse_args()

    # ==========================================================================
    # Load configuration from YAML or use CLI defaults
    # ==========================================================================
    if args.config:
        print(f"Loading configuration from: {args.config}")
        full_config = load_full_config(args.config)

        # Extract values from YAML config (CLI args override these)
        model_name = args.model or full_config.model.name
        output_dir = args.output or full_config.output.checkpoint_dir
        epochs = (
            args.epochs
            if args.epochs is not None
            else full_config.training.phase2_epochs
        )
        batch_size = (
            args.batch_size
            if args.batch_size is not None
            else full_config.training.phase2_batch_size
        )
        lr = args.lr if args.lr is not None else full_config.training.phase2_lr
        alignment_weight = (
            args.alignment_weight
            if args.alignment_weight is not None
            else full_config.training.phase2_alignment_weight
        )
        device = args.device or full_config.device
        num_workers = (
            args.num_workers
            if args.num_workers is not None
            else full_config.training.num_workers
        )

        # Build timestamps paths from config if not provided
        timestamps_dir = full_config.data.timestamps_dir
        train_split = full_config.data.train_split
        val_split = full_config.data.val_split
        timestamps_path = (
            args.timestamps
            or f"{timestamps_dir}/{train_split}/timestamps_manifest.json"
        )
        val_timestamps_path = (
            args.val_timestamps
            or f"{timestamps_dir}/{val_split}/timestamps_manifest.json"
        )
    else:
        # Use CLI arguments with defaults
        model_name = args.model or "voxlm-2b"
        output_dir = args.output or "./checkpoints"
        epochs = args.epochs if args.epochs is not None else 5
        batch_size = args.batch_size if args.batch_size is not None else 16
        lr = args.lr if args.lr is not None else 5e-5
        alignment_weight = (
            args.alignment_weight if args.alignment_weight is not None else 1.0
        )
        device = args.device or "auto"
        num_workers = args.num_workers if args.num_workers is not None else 8

        # Timestamps paths are required in CLI mode
        if not args.timestamps or not args.val_timestamps:
            parser.error(
                "--timestamps and --val-timestamps are required when not using --config"
            )
        timestamps_path = args.timestamps
        val_timestamps_path = args.val_timestamps

    # Auto-detect device
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    setup_torch_optimizations()

    # Load Phase 1 checkpoint
    print(f"\nLoading Phase 1 checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)

    # Get model config from checkpoint or YAML or pre-defined
    if args.config:
        model_config = load_full_config(args.config).model
    else:
        model_config = checkpoint.get("config") or get_config(model_name)

    model_config.alignment_loss_weight = alignment_weight

    model = VoxLM(model_config)
    model.load_state_dict(checkpoint["model_state_dict"])
    print("Phase 1 model loaded!")
    model.print_trainable_parameters()

    # Load datasets
    print(f"\nLoading timestamp datasets...")
    print(f"  Train: {timestamps_path}")
    print(f"  Val: {val_timestamps_path}")

    train_dataset = TimestampDataset(
        timestamps_path,
        max_audio_length=model_config.max_audio_length,
    )
    val_dataset = TimestampDataset(
        val_timestamps_path,
        max_audio_length=model_config.max_audio_length,
    )

    collate_fn = TimestampCollateFn(model.tokenizer, model_config.max_text_length)

    actual_num_workers = num_workers if device == "cuda" else 0
    pin_memory = device == "cuda"

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=actual_num_workers,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=actual_num_workers,
        collate_fn=collate_fn,
        pin_memory=pin_memory,
    )

    # Optimizer with lower LR for fine-tuning
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr,
        weight_decay=0.01,
    )

    total_steps = len(train_loader) * epochs
    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)

    print(f"\nPhase 2 Training config:")
    print(f"  Config file: {args.config or 'None (using CLI args)'}")
    print(f"  Model: {model_name}")
    print(f"  Batch size: {batch_size}")
    print(f"  Learning rate: {lr}")
    print(f"  Epochs: {epochs}")
    print(f"  Alignment loss weight: {alignment_weight}")
    print(f"  Total steps: {total_steps}")

    trainer = TimestampTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        output_dir=f"{output_dir}/{model_name}",
        alignment_loss_weight=alignment_weight,
    )

    trainer.train(epochs)
    print("\nPhase 2 training complete!")


if __name__ == "__main__":
    main()
